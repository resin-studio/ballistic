\documentclass[runningheads]{llncs}

% \documentclass{article}

\usepackage[
backend=biber,
style=alphabetic,
]{biblatex}

\usepackage{mathpartir}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{stmaryrd}
\usepackage{listings}

\makeatletter % allow us to mention @-commands
\def\arcr{\@arraycr}
\makeatother

\lstset{
    % identifierstyle=\color{violet},
    % textcolor=blue,
    % keywordstyle=\color{blue},
    keywordstyle=\text,
    basicstyle=\ttfamily,
    mathescape=true,
    showspaces=false,
    morekeywords={let, fix, in}
}
\usepackage[utf8]{inputenc}
% \usepackage[T1]{fontenc}


\title{Synthesis of stochastic programs from data}
\author{Thomas Logan}
\institute{University of Texas at Austin}

\begin{document}

\maketitle

\section{Introduction}
The aim of this project is to automatically synthesize  programs from data sets. 
The intended scenario consists of a user who wants to fit a line to some data.
It is quite easy to encode an equation
from inputs to outputs in a typical programming language, such as 
\[ f(x) = m x + b \] 
.

However, current programming languages do not provide a compact notation 
for representing the requirement that the weights $m$ and $b$ are learned from data.
\textit{Pyro} \cite{} is a language embedded in \textit{Python} \cite{} for 
expressing the requirements of learning weights from data 
with probabilities for measuring uncertainty in the learned result. 
In Pyro, the simplest way to represent the above linear equation is 

\begin{lstlisting}[language=Python]
def model(xs, ys=None):
    m = pyro.param("m", torch.tensor(0.))
    b = pyro.param("b", torch.tensor(0.))
    
    with pyro.plate("data", len(xs)):
        return pyro.sample("ys", 
            dist.Normal(m * xs + b, 1.), obs=ys)
\end{lstlisting}
.

Clearly, there is a vast increase in notational noise, within which the essence of the idea is buried. 

To reduce the notational noise, this project introduces a simple programming language 
that allows expressing learnable models in a succinct notation. In this new language,
we can express a model for learning a linear equation as 

\begin{lstlisting}[language=Python]
x => {y | x, y : data}
    m $\sim$ normal(0., 1.); 
    b $\sim$ normal(0., 1.); 
    normal(m * x + b, 1)
\end{lstlisting}
.

The notation of our new language is is far more compact than Pyro's. 
It is only slightly more complicated than the bare linear equation, which is
due to annotating information about prior beliefs on the weights, the data, and 
distribution of the data in relation to the equation. 

Unfortunately, this solution leaves open another problem. How do we conjure up some 
initial belief about the distributions and the shape of the equation? 

To avoid needing to specify these details, we simplify the specification further, such that the 
user can simply state the data and its relation to inputs and outputs. 

\begin{lstlisting}[language=Python]
x => {y | x, y : data}
\end{lstlisting}
.

The first part of the solution compiles a program in the DSL into a Pyro model and applies Pyro's 
\textit{stochastic variational inference} (SVI) algorithm \cite{} to learn posterior distributions for the weights. 

The second part of the solution encodes the DSL grammar and semantics into
an logical formula in Z3 \cite{} and uses Z3's \textit{satisfiability modulo theories} (SMT) \cite{} algorithm to search
for a program with priors.  
The particular technique for constructing the search space from the grammar and semantics 
is based on that of Ellis et al \cite{}.
Unfortunately, at the time of this writing, Z3 does not terminate in a reasonable amount of time, 
despite using an encoding very similar to that of Ellis et al. 
After finding the program, the first part may be used to learn posteriors. 


\section{Syntax}

The programs in the domain specific language are constructed using the syntax:
\[
  \begin{array}{l @{} l}
    f &{} ::= p \Rightarrow s\ b \\
    s &{} ::= \{ x \ |\ p : \text{data} \} \\
    p &{} ::= x  \ |\ x,\ p \\
    b &{} ::= x\ \sim\ d\ ;\ b \ |\ x\ \#\ n\ \sim\ d\ ;\ b \ |\ d \\ 
    d &{} ::= 
        \text{normal}(e, e) \ |\ 
        \text{lognorm}(e, e) \ |\ 
        \text{uniform}(e, e) \ |\ 
        \text{halfnorm}(e) \ |\ 
        @(e) \\ 
    e &{} ::= q\ \vec{r} \\
    r &{} ::= +\ q \ |\ -\ q \\
    q &{} ::= a\ \vec{z} \\
    z &{} ::= *\ q \ |\ /\ q \\
    a &{} ::= (e) \ |\ x[e] \ |\ \text{mean}(x) \\
  \end{array}
\]
.

\section{Compiling programs into Pyro}
 The /  models and gradient descent

\section{Encoding language into Z3}
 The / formulas and symbolic reasoning  


\end{document}